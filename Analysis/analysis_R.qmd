---
title: "Psychometric Comparison of Delay Discounting Measures"
subtitle: "Replication of Wan et al. (2023), Behavioural Processes"
author: "Haoran (Matt) Wan"
date: "today"
format: 
  html:
    toc: true
    code-fold: false
    self-contained: true
    theme: cosmo
    mainfont: "Garamond"
execute:
  warning: false
  message: false
engine: knitr
---

## Project Objective

This document provides the complete R code to replicate the analyses from the publication:

> Wan, H., Myerson, J., & Green, L. (2023). Individual differences in degree of discounting: Do different procedures and measures assess the same construct?. *Behavioural Processes*, *208*, 104864. https://doi.org/10.1016/j.beproc.2023.104864

The central research question is methodological: do the two most prominent procedures for measuring delay discounting—the **Adjusting-Amount (Adj-Amt)** procedure and the **Monetary Choice Questionnaire (MCQ)**—actually measure the same underlying construct?

This analysis tests the convergent validity of these two measures using data from two large online samples (Prolific and MTurk). The findings are crucial for determining whether results from studies using different procedures can be validly compared.


## Analysis Workflow

The analysis is organized into the following sections:

1.  **Setup & Data Processing**: Loads all R packages, imports the raw data, and processes it into analysis-ready formats. This includes calculating both theoretical (`log k`) and atheoretical (Area under the Curve / choice proportion) discounting measures for each individual.
2.  **Data Quality & Reliability**: Replicates the initial analyses from the paper that establish the quality and internal consistency of the data from both procedures and participant samples.
3.  **Convergent Validity (Correlation Analysis)**: Replicates the core correlational analyses (Table 3) to test the primary hypothesis that measures from the Adj-Amt and MCQ procedures are highly correlated.
4.  **Procedural & Sample Differences (ANOVA)**: Replicates the mixed-effects ANOVA used to test for systematic differences in measured discounting rates between the two procedures and the two online samples.

```{r setup_and_process}
# --- 1. SETUP: LOAD PACKAGES AND PROCESS DATA ---

# This section prepares the R environment, loads the raw dataset, and transforms
# it into an analysis-ready format. This includes calculating the two primary
# outcome measures for each participant: an atheoretical measure (AuC or choice
# count) and a theoretical measure (log k).

# --- 1.1 Load R Packages ---
library(minpack.lm) # For robust non-linear least squares regression (nlsLM)
library(glmmTMB)    # For mixed-effects modeling (used in ANOVA)
library(emmeans)    # For post-hoc tests
library(psych)      # For psychological research tools (e.g., geometric.mean)
library(readr)      # For fast CSV file reading
library(tidyr)      # For tidying data
library(multcomp)   # For linear contrast
library(dplyr)      # For data manipulation and wrangling

# --- 1.2 Load Raw Data ---
raw_data <- read_csv("AdjAmt_MCQ.csv") |> select(-1)

# --- 1.3 Calculate Individual-Level Discounting Measures ---

#' @title Calculate Area Under the Curve
#' @description Calculates area under a curve using the trapezoidal rule.
#' @param x A numeric vector of x-coordinates (e.g., normalized delays).
#' @param y A numeric vector of y-coordinates (e.g., subjective values).
#' @return A single numeric value representing the area.
calculate_auc <- function(x, y) {
  sum(diff(x) * (y[-1] + y[-length(y)])) / 2
}

# Process the Adjusting-Amount (Adj-Amt) data
adj_amt_summary <- raw_data |>
  filter(procedure == "aa" & iv != 730) |>
  group_by(id, procedure, amt) |>
  summarise(
    # Atheoretical measure: Area under the Curve (AuC)
    atheoretical = calculate_auc(iv / 180, value),
    # Theoretical measure: log(k) from a simple hyperbola
    theoretical = tryCatch({
      # Fit a non-linear model to estimate the k parameter for each individual
      fit <- nlsLM(log(value) ~ -log(1 + exp(k) * iv), start = list(k = -4))
      coef(fit)[['k']]
    }, error = function(e) NA), # Return NA if the model fails to converge
    .groups = 'drop'
  )

# Process the Monetary Choice Questionnaire (MCQ) data
mcq_summary <- raw_data |>
  filter(procedure == "mcq") |>
  group_by(id, amt) |>
  summarise(
    # Atheoretical measure: Total number of delayed choices
    atheoretical = sum(value),
    # Theoretical measure: log(k) estimated from the choice pattern
    theoretical = {
      # This block uses the standard scoring algorithm for the MCQ to find
      # the k value that makes the participant's choices most consistent.
      choices <- value
      k_values <- iv
      
      if (all(choices == 1)) {
        # Edge case: If always patient, assign the smallest possible k
        log(0.00016)
      } else if (all(choices == 0)) {
        # Edge case: If always impatient, assign the largest possible k
        log(0.25)
      } else {
        # Standard case: Find the k value where the participant switches preference
        n_consistent <- sapply(k_values, function(k_level) {
          sum((choices == 0 & k_values <= k_level) | (choices == 1 & k_values > k_level))
        })
        # The indifference point is the geometric mean of all k-values
        # that maximize choice consistency.
        indifference_ks <- k_values[which(n_consistent == max(n_consistent))]
        log(geometric.mean(indifference_ks))
      }
    },
    .groups = 'drop'
  ) |>
  mutate(procedure = "mcq") |>
  distinct(id, amt, .keep_all = TRUE)

# --- 1.4 Combine into Final Analysis DataFrame ---
# Bind the processed data from both procedures and add participant info
individual_level_data <- bind_rows(adj_amt_summary, mcq_summary) |>
  left_join(distinct(select(raw_data, id, provider)), by = "id")
```

::: {.callout-note}
## Note on `log k` Estimation
The theoretical discounting parameter (`log k`) is estimated in this script using nonlinear least-squares (`nlsLM`) for the Adjusting-Amount data and a direct scoring algorithm for the MCQ. These methods are computationally efficient for this environment. The values in the original publication were derived from a fully Bayesian analysis, which may lead to minor numerical differences in the point estimates. This does not alter the study's conclusions regarding the high correlation between measures.
:::

---

## 2. Data Quality and Reliability Analysis

This section replicates the initial analyses from the paper that establish the quality and internal consistency of the data. This is a crucial step to ensure the data is valid before testing the primary hypotheses.

### 2.1 Group-Level Model Fits & The Amount Effect

First, we assess how well established mathematical models of choice describe the aggregated data from each group. High $R^2$ values indicate that participants' choices were systematic and not random.

Second, we test for the **"amount effect,"** a benchmark finding in discounting research where delayed rewards are discounted less steeply as their amount increases. Confirming this effect serves as a critical validity check for the dataset.

```{r group_analysis}
# --- 2.1.1 Adjusting-Amount Procedure: Data Quality ---

# Create a summary of the data at the group level for fitting models
group_level_summary <- raw_data |>
  group_by(provider, procedure, amt, iv) |>
  mutate(iv = ifelse(procedure == "mcq", log(iv), iv)) |>
  summarise(
    mean_sv = mean(value), 
    median_sv = median(value), 
    .groups = 'drop'
  )

# Calculate R-squared for group-level hyperboloid model fits
cat("--- Adj-Amt: Group-level Hyperboloid Model Fit (R-squared) ---\n")
r2_adj_amt <- group_level_summary |>
  filter(procedure == "aa") |>
  group_by(provider, amt) |>
  reframe({
    # Fit a non-linear model to the median data for each group
    fit <- nlsLM(median_sv ~ 1 / (1 + exp(k) * iv)^s, 
                   start = list(k = -4, s = 1), data = cur_data())
    # Calculate R-squared manually
    rss <- sum(residuals(fit)^2)
    tss <- sum((cur_data()$median_sv - mean(cur_data()$median_sv))^2)
    tibble(r_squared = 1 - (rss / tss))
  }) |>
  pivot_wider(names_from = provider, values_from = r_squared)

print(as.data.frame(r2_adj_amt), digits = 3)


# Test for the amount effect using a beta mixed-effects regression
cat("\n--- Adj-Amt: Amount Effect (Beta Regression p-values) ---\n")
adj_amt_amount_effect <- individual_level_data |>
  filter(procedure == "aa") |>
  group_by(provider) |>
  summarise(
    # Fit a beta regression model because the outcome (AuC) is a proportion (0 to 1)
    p_value = summary(glht(
      glmmTMB(atheoretical ~ as.factor(amt) + (1 | id), family = beta_family(), data = cur_data()),
      linfct = matrix(c(-1, 0, 1), nrow = 1) # Contrast to test for a linear trend across amounts
    ))$test$pvalues[1],
    .groups = 'drop'
  )

print(adj_amt_amount_effect)


# --- 2.1.2 Monetary Choice Questionnaire (MCQ): Data Quality ---

# Calculate R-squared for group-level logistic growth model fits
cat("\n\n--- MCQ: Group-level Logistic Growth Model Fit (R-squared) ---\n")
r2_mcq <- group_level_summary |>
  filter(procedure == "mcq") |>
  group_by(provider, amt) |>
  reframe({
    fit <- nlsLM(mean_sv ~ 1 / (1 + exp(-(iv - x) * r)), 
                   start = list(x = -4, r = 1), data = cur_data())
    rss <- sum(residuals(fit)^2)
    tss <- sum((cur_data()$mean_sv - mean(cur_data()$mean_sv))^2)
    tibble(r_squared = 1 - (rss / tss))
  }) |>
  pivot_wider(names_from = provider, values_from = r_squared)

print(as.data.frame(r2_mcq), digits = 3)


# Test for the amount effect using a binomial mixed-effects regression
cat("\n--- MCQ: Amount Effect (Logistic Regression p-values) ---\n")
mcq_amount_effect <- individual_level_data |>
  filter(procedure == "mcq") |>
  group_by(provider) |>
  summarise(
    # Fit a binomial (logistic) regression because the outcome is a count of choices
    p_value = summary(glht(
      glmmTMB(cbind(atheoretical, 9 - atheoretical) ~ as.factor(amt) + (1 | id), 
              family = binomial, data = cur_data()),
      linfct = matrix(c(contr.poly(3)[,1]), nrow = 1) # Contrast for a linear trend
    ))$test$pvalues[1],
    .groups = 'drop'
  )
  
print(mcq_amount_effect)
```

---

### 2.2 Reliability and Internal Consistency

These analyses evaluate the psychometric properties of the discounting measures, corresponding to **Tables 1 and 2** in the paper. We assess two key aspects of reliability:

1.  **Alternate-Forms Reliability**: We test whether individuals' discounting behavior is consistent across different reward amounts within the same procedure. High positive correlations (e.g., between a participant's score for a $30 reward and their score for an $80 reward) indicate that the measure is internally consistent.
2.  **Convergent Validity of Measures**: We test whether the atheoretical (e.g., AuC) and theoretical (e.g., `log k`) scoring methods capture the same information. High (negative) correlations between these two measures confirm that they are assessing the same underlying construct.

```{r reliability_analysis}
# --- 2.2.1 Alternate-Forms Reliability (Within-Procedure) ---

# These analyses test whether each measure (atheoretical and theoretical) is
# internally consistent across different reward amounts.

cat("--- Adj-Amt: Within-Measure Correlations ---\n")
# Filter for Adj-Amt data, pivot to wide format, and calculate correlations
adj_amt_reliability <- individual_level_data |>
  filter(procedure == "aa") |>
  select(id, provider, amt, atheoretical, theoretical) |>
  pivot_wider(names_from = amt, values_from = c(atheoretical, theoretical)) |>
  group_by(provider) |>
  summarise(
    # Atheoretical (AuC) correlations
    AuC = list(cor(select(cur_data(), starts_with("atheoretical")), use = "complete.obs")),
    # Theoretical (log k) correlations
    LogK = list(cor(select(cur_data(), starts_with("theoretical")), use = "complete.obs"))
  )
# Print the correlation matrices for each provider
print(adj_amt_reliability$AuC, digits = 3)
print(adj_amt_reliability$LogK, digits = 3)


cat("\n\n--- MCQ: Within-Measure Correlations ---\n")
# Filter for MCQ data, pivot to wide format, and calculate correlations
mcq_reliability <- individual_level_data |>
  filter(procedure == "mcq") |>
  select(id, provider, amt, atheoretical, theoretical) |>
  pivot_wider(names_from = amt, values_from = c(atheoretical, theoretical)) |>
  group_by(provider) |>
  summarise(
    # Atheoretical (Choice Count) correlations
    ChoiceCount = list(cor(select(cur_data(), starts_with("atheoretical")), use = "complete.obs")),
    # Theoretical (log k) correlations
    LogK = list(cor(select(cur_data(), starts_with("theoretical")), use = "complete.obs"))
  )
# Print the correlation matrices for each provider
print(mcq_reliability$ChoiceCount, digits = 3)
print(mcq_reliability$LogK, digits = 3)


# --- 2.2.2 Convergent Validity of Measures (Between-Measure) ---

# This analysis tests if the atheoretical and theoretical scoring methods are highly
# correlated, which would confirm they measure the same construct.
# The correlation should be strongly negative, as a lower AuC (more discounting)
# corresponds to a higher log k (more discounting).

cat("\n\n--- Correlation Between Atheoretical and Theoretical Measures ---\n")
measure_convergence <- individual_level_data |>
  group_by(provider, procedure, amt) |>
  summarise(
    correlation = cor(atheoretical, theoretical, use = "complete.obs"), 
    .groups = 'drop'
  ) |>
  pivot_wider(names_from = amt, values_from = correlation, names_prefix = "amt_")

print(measure_convergence)
```

---

### 2.3 Convergent Validity: Comparing the Two Procedures

This final analysis addresses the primary research question: **do the Adjusting-Amount and MCQ procedures measure the same underlying construct?** To test this, we assess their convergent validity by correlating the discounting measures derived from each procedure.

If both procedures are indeed measuring the same trait (e.g., "impulsivity" or "financial patience"), then an individual's score on the Adj-Amt procedure should be highly correlated with their score on the MCQ. This analysis corresponds to **Table 3** in the paper.

```{r validity_analysis}
# --- 2.3 Convergent Validity: Between-Procedure Correlations ---

# This analysis correlates the measures from the Adj-Amt procedure with those
# from the MCQ to test if they measure the same underlying construct.

# First, prepare a wide-format dataframe where each row is a participant,
# and columns represent the measures from both procedures for the common amounts ($30, $80).
wide_format_for_correlation <- individual_level_data |>
  # Isolate the two common reward amounts from each procedure
  filter((procedure == "aa" & amt %in% c(1, 2)) | (procedure == "mcq" & amt %in% c(1, 3))) |>
  # Create a consistent amount label for joining
  mutate(amount_label = ifelse(amt == 1, "$30", "$80")) |>
  select(-amt) |> 
  pivot_wider(
    names_from = procedure, 
    values_from = c(atheoretical, theoretical)
  )

# Now, calculate the correlations for each sample and amount combination.
cat("--- Between-Procedure Correlations (Atheoretical Measures) ---\n")
atheoretical_correlations <- wide_format_for_correlation |>
  group_by(provider, amount_label) |> 
  summarise(
    correlation = cor(atheoretical_aa, atheoretical_mcq, use = "complete.obs"),
    .groups = 'drop'
  ) |>
  pivot_wider(names_from = provider, values_from = correlation)

print(atheoretical_correlations)


cat("\n--- Between-Procedure Correlations (Theoretical Measures) ---\n")
theoretical_correlations <- wide_format_for_correlation |>
  group_by(provider, amount_label) |> 
  summarise(
    # Note: Correlation between log(k) values should be positive
    correlation = cor(theoretical_aa, theoretical_mcq, use = "complete.obs"),
    .groups = 'drop'
  ) |>
  pivot_wider(names_from = provider, values_from = correlation)

print(theoretical_correlations)
```

---

### 3. Comparing Systematic Differences in Discounting

While the correlation analyses confirm that the two procedures measure the same construct, a crucial practical question remains: **are the measures interchangeable?** That is, do the two procedures and two online samples produce systematically different absolute values of discounting?

To answer this, we replicate the final analysis from the paper, a mixed-effects ANOVA. This model examines the main effects and interactions of **experimental procedure** (Adj-Amt vs. MCQ), **participant sample** (Prolific vs. MTurk), and **reward amount** on the estimated `log k` values. Post-hoc tests are then used to explore the significant interaction found in the paper, pinpointing exactly where the differences lie.

```{r anova_analysis}
# --- 3.1 Mixed-Effects ANOVA on Theoretical (log k) Measures ---

# This analysis uses a linear mixed-effects model to replicate the ANOVA from
# the paper. A mixed-effects model is appropriate because each participant
# provides multiple data points (i.e., repeated measures). This model accounts
# for that non-independence by including a random intercept for each participant.

# Note: The lme4 and lmerTest packages are required for this analysis.
library(lme4)
library(lmerTest)

# Prepare the data for the model, filtering for the common reward amounts
anova_df <- individual_level_data |>
  filter((procedure == "aa" & amt %in% c(1, 2)) | (procedure == "mcq" & amt %in% c(1, 3))) |>
  mutate(
    amt = factor(ifelse(amt %in% c(1), "$30", "$80")),
    procedure = factor(procedure),
    provider = factor(provider),
    id = factor(id)
  )

# Fit the linear mixed-effects model
logk_mixed_model <- lmer(theoretical ~ provider * amt * procedure + (1 | id), data = anova_df)

# Print the ANOVA table with Type III sums of squares
cat("--- Mixed-Effects ANOVA: Comparing log k Values ---\n")
print(anova(logk_mixed_model))


# --- 3.2 Post-Hoc Contrasts ---

# The ANOVA revealed a significant interaction between procedure and amount.
# We use post-hoc tests to compare the log k values from the two procedures
# at each reward amount to understand this interaction.

cat("\n\n--- Post-Hoc Contrasts for Procedure x Amount Interaction ---\n")
# Create an emmeans object from the fitted model
emm_obj <- emmeans(logk_mixed_model, ~ procedure | amt)
# Perform pairwise comparisons between procedures for each amount
print(pairs(emm_obj, adjust = "holm"))
```