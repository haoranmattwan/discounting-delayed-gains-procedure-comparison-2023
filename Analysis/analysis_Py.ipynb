{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ebbf03",
   "metadata": {},
   "source": [
    "# Psychometric Comparison of Delay Discounting Measures\n",
    "### A Python Replication of Wan et al. (2023), *Behavioural Processes*\n",
    "\n",
    "---\n",
    "\n",
    "### Project Objective\n",
    "\n",
    "This notebook provides a complete, reproducible Python workflow for the analyses presented in the following publication:\n",
    "\n",
    "> Wan, H., Myerson, J., & Green, L. (2023). Individual differences in degree of discounting: Do different procedures and measures assess the same construct?. *Behavioural Processes*, *208*, 104864. https://doi.org/10.1016/j.beproc.2023.104864\n",
    "\n",
    "The central research question is methodological: do the two most prominent procedures for measuring financial patience—the **Adjusting-Amount (Adj-Amt)** procedure and the **Monetary Choice Questionnaire (MCQ)**—actually measure the same underlying construct? This analysis tests the convergent validity of these two measurement tools using data from two large online samples.\n",
    "\n",
    "### Analysis Workflow\n",
    "\n",
    "The analysis is structured into the following sections:\n",
    "\n",
    "1.  **Setup & Data Processing**: Loads libraries, defines helper functions, and processes the raw data into an analysis-ready format. This includes calculating both theoretical (`log k`) and atheoretical (Area under the Curve / choice count) discounting measures for each individual.\n",
    "2.  **Data Quality & Reliability**: Replicates the initial analyses establishing the quality and internal consistency of the data from both procedures.\n",
    "3.  **Convergent Validity (Correlation Analysis)**: Replicates the core correlational analyses to test the primary hypothesis that measures from the two procedures are highly correlated.\n",
    "4.  **Procedural & Sample Differences (ANOVA)**: Replicates the mixed-effects ANOVA used to test for systematic differences in the measured discounting rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e456b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Setup ---\n",
    "#\n",
    "# This cell installs the required Python packages for the analysis.\n",
    "# Uncomment and run this cell only if you are setting up a new environment.\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install pandas numpy scipy statsmodels openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77659b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP: IMPORTS, FUNCTIONS, AND DATA PROCESSING ---\n",
    "\n",
    "# --- 1.1 Load Libraries ---\n",
    "\n",
    "# Core data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Statistical modeling and analysis\n",
    "from lmfit import Model\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import gmean\n",
    "\n",
    "# Suppress warnings for a cleaner final report\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set pandas display options for consistent formatting\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "\n",
    "# --- 1.2 Custom Helper Functions ---\n",
    "\n",
    "def fit_nls_logk(df):\n",
    "    \"\"\"\n",
    "    Fits a simple hyperbola to individual Adj-Amt data to estimate log(k).\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): A dataframe for a single participant and amount,\n",
    "                           containing 'iv' (delay) and 'value' (subjective value).\n",
    "    Returns:\n",
    "        float: The estimated log(k) parameter.\n",
    "    \"\"\"\n",
    "    def model_func(iv, k):\n",
    "        return -np.log(1 + np.exp(k) * iv)\n",
    "    \n",
    "    try:\n",
    "        popt, _ = curve_fit(model_func, df['iv'], np.log(df['value']), p0=[-4])\n",
    "        return popt[0]\n",
    "    except RuntimeError:\n",
    "        return np.nan # Return NA if the model fails to converge\n",
    "\n",
    "def score_mcq_logk(df):\n",
    "    \"\"\"\n",
    "    Calculates the theoretical log(k) for MCQ data using Kirby's (1999)\n",
    "    consistency-checking scoring algorithm.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): A dataframe for a single participant and amount,\n",
    "                           containing 'iv' (k_values) and 'value' (choices).\n",
    "    Returns:\n",
    "        float: The estimated log(k) parameter.\n",
    "    \"\"\"\n",
    "    choices = df['value'].values\n",
    "    k_values = df['iv'].values\n",
    "    \n",
    "    # Handle edge cases for participants who never switch preference\n",
    "    if all(choices == 1): return np.log(0.00016) \n",
    "    if all(choices == 0): return np.log(0.25)   \n",
    "    \n",
    "    # For participants who switch, find the k-value that maximizes consistency\n",
    "    n_consistent = [\n",
    "        sum((choices == 0) & (k_values <= k) | (choices == 1) & (k_values > k))\n",
    "        for k in k_values\n",
    "    ]\n",
    "    max_consistency = np.max(n_consistent)\n",
    "    indifference_ks = k_values[np.where(n_consistent == max_consistency)]\n",
    "    \n",
    "    # The estimated indifference point is the geometric mean of the k-values\n",
    "    # that produce the most consistent choice pattern.\n",
    "    return np.log(gmean(indifference_ks))\n",
    "\n",
    "# --- Define mathematical functions for curve fitting ---\n",
    "def hyperboloid_model(iv, k, s):\n",
    "    \"\"\"Hyperboloid discounting function for group-level Adj-Amt data.\"\"\"\n",
    "    return 1 / (1 + np.exp(k) * iv)**s\n",
    "\n",
    "def logistic_growth_model(iv_log, x, r):\n",
    "    \"\"\"Logistic growth function for group-level MCQ data.\"\"\"\n",
    "    return 1 / (1 + np.exp(-(iv_log - x) * r))\n",
    "\n",
    "def test_amount_effect(df_provider, procedure):\n",
    "    \"\"\"\n",
    "    Fits a GLM with clustered standard errors and performs a Wald test\n",
    "    to check for a linear trend in discounting across reward amounts.\n",
    "    \"\"\"\n",
    "    # Define a contrast matrix for a linear trend (Amount 3 > Amount 1)\n",
    "    contrast_matrix = np.array([[-1, 0, 1]])\n",
    "    \n",
    "    if procedure == 'aa':\n",
    "        # For proportions, clip values to (0, 1) and use a Binomial GLM\n",
    "        # This approximates a beta regression when data hits the boundaries.\n",
    "        df_provider['atheoretical_adj'] = np.clip(df_provider['atheoretical'], 1e-5, 1 - 1e-5)\n",
    "        formula = \"atheoretical_adj ~ C(amt)\"\n",
    "        model = smf.glm(formula, data=df_provider, family=sm.families.Binomial())\n",
    "        \n",
    "    elif procedure == 'mcq':\n",
    "        # For counts, create a proportion and use a weighted Binomial GLM\n",
    "        df_provider['prop_delayed'] = df_provider['atheoretical'] / 9.0\n",
    "        formula = \"prop_delayed ~ C(amt)\"\n",
    "        model = smf.glm(formula, data=df_provider, family=sm.families.Binomial(),\n",
    "                        weights=np.repeat(9, len(df_provider)))\n",
    "        \n",
    "    # Fit the model with standard errors clustered by participant ID\n",
    "    fit = model.fit(cov_type='cluster', cov_kwds={'groups': df_provider['id']})\n",
    "    wald_test = fit.wald_test(contrast_matrix)\n",
    "    return wald_test.pvalue\n",
    "\n",
    "# --- 1.3 Load and Process Data ---\n",
    "\n",
    "# Load the raw data from a CSV file\n",
    "raw_df = pd.read_csv(\"AdjAmt_MCQ.csv\").iloc[:, 1:]\n",
    "print(\"Raw data loaded successfully.\")\n",
    "\n",
    "# --- Process Adj-Amt Data ---\n",
    "adj_amt_summary = raw_df[\n",
    "    (raw_df['procedure'] == \"aa\") & (raw_df['iv'] != 730)\n",
    "].groupby(['id', 'procedure', 'amt']).apply(lambda g: pd.Series({\n",
    "    # Atheoretical measure: Area Under the Curve (AuC) using the trapezoidal rule\n",
    "    'atheoretical': np.trapz(y=g['value'], x=g['iv'] / 180),\n",
    "    # Theoretical measure: log(k) from a non-linear squares fit\n",
    "    'theoretical': fit_nls_logk(g)\n",
    "})).reset_index()\n",
    "\n",
    "# --- Process MCQ Data ---\n",
    "mcq_summary = raw_df[\n",
    "    raw_df['procedure'] == \"mcq\"\n",
    "].groupby(['id', 'amt']).apply(\n",
    "    lambda g: pd.Series({\n",
    "        # Atheoretical measure: Total number of delayed choices\n",
    "        'atheoretical': g['value'].sum(),\n",
    "        # Theoretical measure: log(k) from the scoring algorithm\n",
    "        'theoretical': score_mcq_logk(g)\n",
    "    })\n",
    ").reset_index()\n",
    "mcq_summary['procedure'] = 'mcq'\n",
    "\n",
    "# --- Combine into Final Analysis DataFrame ---\n",
    "provider_map = raw_df[['id', 'provider']].drop_duplicates()\n",
    "individual_level_df = pd.concat([adj_amt_summary, mcq_summary]).merge(provider_map, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524a362",
   "metadata": {},
   "source": [
    "## 2. Data Quality and Reliability Analysis\n",
    "\n",
    "This section replicates the initial analyses from the paper that establish the quality and internal consistency of the data. This is a crucial step to ensure the data is valid before testing the primary hypotheses.\n",
    "\n",
    "### 2.1 Group-Level Model Fits & The Amount Effect\n",
    "\n",
    "First, we assess how well established mathematical models of choice describe the aggregated data from each group. High R-squared ($R^2$) values indicate that participants' choices were systematic and not random, conforming to theoretical expectations.\n",
    "\n",
    "Second, we test for the **\"amount effect,\"** a benchmark finding in discounting research where delayed rewards are discounted less steeply as their amount increases. Confirming this effect serves as a critical validity check for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19926516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Group-Level Nonlinear Model Fit (R-squared) ---\n",
      "\n",
      "Adj-Amt R-squared:\n",
      "provider  MTurk  Prolific\n",
      "amt                      \n",
      "1         0.882     0.980\n",
      "2         0.996     0.982\n",
      "3         0.987     0.979\n",
      "\n",
      "MCQ R-squared:\n",
      "provider  MTurk  Prolific\n",
      "amt                      \n",
      "1         0.991     0.991\n",
      "2         0.982     0.984\n",
      "3         0.965     0.996\n",
      "\n",
      "\n",
      "--- Adj-Amt: Amount Effect (p-values) ---\n",
      "                         p_value\n",
      "provider                        \n",
      "MTurk     1.4083481218328474e-21\n",
      "Prolific   8.073663339395418e-07\n",
      "\n",
      "--- MCQ: Amount Effect (p-values) ---\n",
      "                         p_value\n",
      "provider                        \n",
      "MTurk     1.2768358690089668e-64\n",
      "Prolific  1.2601763159855179e-30\n"
     ]
    }
   ],
   "source": [
    "# --- 2.1.1 Group-Level Model Fit Assessment ---\n",
    "\n",
    "# --- Create aggregated dataframe for group-level analysis ---\n",
    "group_level_summary = raw_df.groupby(['provider', 'procedure', 'amt', 'iv']).agg(\n",
    "    mean_value=('value', 'mean'),\n",
    "    median_value=('value', 'median')\n",
    ").reset_index()\n",
    "group_level_summary['iv_log'] = np.log(group_level_summary['iv'])\n",
    "\n",
    "# --- Calculate R-squared for group fits ---\n",
    "print(\"--- Group-Level Nonlinear Model Fit (R-squared) ---\")\n",
    "\n",
    "# Adj-Amt Procedure\n",
    "aa_model_fit = Model(hyperboloid_model)\n",
    "r2_adj_amt = group_level_summary[group_level_summary['procedure'] == 'aa'].groupby(['provider', 'amt']).apply(\n",
    "    lambda g: aa_model_fit.fit(g['median_value'], iv=g['iv'], k=-4, s=1).rsquared\n",
    ").unstack(level='provider')\n",
    "print(\"\\nAdj-Amt R-squared:\")\n",
    "print(r2_adj_amt)\n",
    "\n",
    "# MCQ Procedure\n",
    "mcq_model_fit = Model(logistic_growth_model)\n",
    "r2_mcq = group_level_summary[group_level_summary['procedure'] == 'mcq'].groupby(['provider', 'amt']).apply(\n",
    "    lambda g: mcq_model_fit.fit(g['mean_value'], iv_log=g['iv_log'], x=-4, r=1).rsquared\n",
    ").unstack(level='provider')\n",
    "print(\"\\nMCQ R-squared:\")\n",
    "print(r2_mcq)\n",
    "\n",
    "\n",
    "# --- 2.1.2 Amount Effect Test ---\n",
    "\n",
    "# --- Run tests for Adj-Amt procedure ---\n",
    "print(\"\\n\\n--- Adj-Amt: Amount Effect (p-values) ---\")\n",
    "adj_amt_effect = individual_level_df[\n",
    "    individual_level_df['procedure'] == 'aa'\n",
    "].groupby('provider').apply(\n",
    "    test_amount_effect, procedure='aa'\n",
    ").rename('p_value').to_frame()\n",
    "print(adj_amt_effect)\n",
    "\n",
    "# --- Run tests for MCQ procedure ---\n",
    "print(\"\\n--- MCQ: Amount Effect (p-values) ---\")\n",
    "mcq_effect = individual_level_df[\n",
    "    individual_level_df['procedure'] == 'mcq'\n",
    "].groupby('provider').apply(\n",
    "    test_amount_effect, procedure='mcq'\n",
    ").rename('p_value').to_frame()\n",
    "print(mcq_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65d29b7",
   "metadata": {},
   "source": [
    "## 2.2 Reliability and Validity Correlations\n",
    "\n",
    "These analyses replicate the correlation tables from the paper (**Tables 1, 2, and 3**), providing a comprehensive psychometric evaluation of the two discounting procedures.\n",
    "\n",
    "1.  **Alternate-Forms Reliability**: First, we test whether individuals' discounting behavior is consistent across different reward amounts **within** the same procedure. High positive correlations indicate that the measures are internally consistent and reliable.\n",
    "2.  **Convergent Validity of Measures**: Next, we test whether the atheoretical (e.g., AuC) and theoretical (e.g., `log k`) scoring methods capture the same information. High (negative) correlations between these two measures confirm they are assessing the same underlying construct **within** each procedure.\n",
    "3.  **Convergent Validity of Procedures**: Finally, we address the study's primary hypothesis by correlating the measures **between** the two different procedures. If both the Adj-Amt and MCQ tasks are measuring the same trait, their respective measures should be highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c98cc94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Adj-Amt: Within-Measure Correlations ---\n",
      "\n",
      "Atheoretical (AuC) Correlations:\n",
      "amt                            1      2      3\n",
      "provider              amt                     \n",
      "MTurk    atheoretical 1    1.000  0.877  0.841\n",
      "                      2    0.877  1.000  0.879\n",
      "                      3    0.841  0.879  1.000\n",
      "         theoretical  1   -0.973 -0.863 -0.826\n",
      "                      2   -0.866 -0.979 -0.852\n",
      "                      3   -0.852 -0.883 -0.982\n",
      "Prolific atheoretical 1    1.000  0.841  0.737\n",
      "                      2    0.841  1.000  0.870\n",
      "                      3    0.737  0.870  1.000\n",
      "         theoretical  1   -0.963 -0.806 -0.697\n",
      "                      2   -0.840 -0.958 -0.803\n",
      "                      3   -0.759 -0.864 -0.956\n",
      "\n",
      "Theoretical (log k) Correlations:\n",
      "amt                            1      2      3\n",
      "provider              amt                     \n",
      "MTurk    atheoretical 1   -0.973 -0.866 -0.852\n",
      "                      2   -0.863 -0.979 -0.883\n",
      "                      3   -0.826 -0.852 -0.982\n",
      "         theoretical  1    1.000  0.874  0.840\n",
      "                      2    0.874  1.000  0.865\n",
      "                      3    0.840  0.865  1.000\n",
      "Prolific atheoretical 1   -0.963 -0.840 -0.759\n",
      "                      2   -0.806 -0.958 -0.864\n",
      "                      3   -0.697 -0.803 -0.956\n",
      "         theoretical  1    1.000  0.839  0.733\n",
      "                      2    0.839  1.000  0.832\n",
      "                      3    0.733  0.832  1.000\n",
      "\n",
      "\n",
      "--- MCQ: Within-Measure Correlations ---\n",
      "\n",
      "Atheoretical (Choice Count) Correlations:\n",
      "amt                            1      2      3\n",
      "provider              amt                     \n",
      "MTurk    atheoretical 1    1.000  0.897  0.890\n",
      "                      2    0.897  1.000  0.925\n",
      "                      3    0.890  0.925  1.000\n",
      "         theoretical  1   -0.966 -0.879 -0.864\n",
      "                      2   -0.894 -0.986 -0.918\n",
      "                      3   -0.863 -0.900 -0.975\n",
      "Prolific atheoretical 1    1.000  0.898  0.837\n",
      "                      2    0.898  1.000  0.882\n",
      "                      3    0.837  0.882  1.000\n",
      "         theoretical  1   -0.996 -0.903 -0.835\n",
      "                      2   -0.898 -1.000 -0.882\n",
      "                      3   -0.833 -0.873 -0.995\n",
      "\n",
      "Theoretical (log k) Correlations:\n",
      "amt                            1      2      3\n",
      "provider              amt                     \n",
      "MTurk    atheoretical 1   -0.966 -0.894 -0.863\n",
      "                      2   -0.879 -0.986 -0.900\n",
      "                      3   -0.864 -0.918 -0.975\n",
      "         theoretical  1    1.000  0.883  0.846\n",
      "                      2    0.883  1.000  0.896\n",
      "                      3    0.846  0.896  1.000\n",
      "Prolific atheoretical 1   -0.996 -0.898 -0.833\n",
      "                      2   -0.903 -1.000 -0.873\n",
      "                      3   -0.835 -0.882 -0.995\n",
      "         theoretical  1    1.000  0.902  0.832\n",
      "                      2    0.902  1.000  0.874\n",
      "                      3    0.832  0.874  1.000\n",
      "\n",
      "\n",
      "--- Correlation Between Atheoretical and Theoretical Measures ---\n",
      "amt                     1      2      3\n",
      "provider procedure                     \n",
      "MTurk    aa        -0.973 -0.979 -0.982\n",
      "         mcq       -0.966 -0.986 -0.975\n",
      "Prolific aa        -0.963 -0.958 -0.956\n",
      "         mcq       -0.996 -1.000 -0.995\n",
      "\n",
      "\n",
      "--- Between-Procedure Correlations (Convergent Validity) ---\n",
      "                       atheoretical_corr  theoretical_corr\n",
      "provider amount_label                                     \n",
      "MTurk    $30                       0.790             0.776\n",
      "         $80                       0.820             0.792\n",
      "Prolific $30                       0.809             0.760\n",
      "         $80                       0.821             0.798\n"
     ]
    }
   ],
   "source": [
    "# --- 2.2.1 Alternate-Forms Reliability (Within-Procedure) ---\n",
    "\n",
    "# Test if each measure is internally consistent across different reward amounts.\n",
    "print(\"--- Adj-Amt: Within-Measure Correlations ---\")\n",
    "# Pivot Adj-Amt data to have amounts as columns for each measure type\n",
    "adj_amt_pivot_wide = individual_level_df[\n",
    "    individual_level_df['procedure'] == 'aa'\n",
    "].pivot_table(index=['id', 'provider'], columns='amt', values=['atheoretical', 'theoretical'])\n",
    "\n",
    "# Calculate and display correlations for each provider and measure\n",
    "adj_amt_reliability = adj_amt_pivot_wide.groupby('provider').corr()\n",
    "print(\"\\nAtheoretical (AuC) Correlations:\")\n",
    "print(adj_amt_reliability.loc[:, ('atheoretical', slice(None))].droplevel(0, axis=1))\n",
    "print(\"\\nTheoretical (log k) Correlations:\")\n",
    "print(adj_amt_reliability.loc[:, ('theoretical', slice(None))].droplevel(0, axis=1))\n",
    "\n",
    "print(\"\\n\\n--- MCQ: Within-Measure Correlations ---\")\n",
    "# Pivot MCQ data to have amounts as columns for each measure type\n",
    "mcq_pivot_wide = individual_level_df[\n",
    "    individual_level_df['procedure'] == 'mcq'\n",
    "].pivot_table(index=['id', 'provider'], columns='amt', values=['atheoretical', 'theoretical'])\n",
    "\n",
    "# Calculate and display correlations for each provider and measure\n",
    "mcq_reliability = mcq_pivot_wide.groupby('provider').corr()\n",
    "print(\"\\nAtheoretical (Choice Count) Correlations:\")\n",
    "print(mcq_reliability.loc[:, ('atheoretical', slice(None))].droplevel(0, axis=1))\n",
    "print(\"\\nTheoretical (log k) Correlations:\")\n",
    "print(mcq_reliability.loc[:, ('theoretical', slice(None))].droplevel(0, axis=1))\n",
    "\n",
    "# --- 2.2.2 Convergent Validity of Measures (Between-Measure) ---\n",
    "\n",
    "# Test if the atheoretical and theoretical scoring methods are highly correlated.\n",
    "print(\"\\n\\n--- Correlation Between Atheoretical and Theoretical Measures ---\")\n",
    "measure_convergence = individual_level_df.groupby(['provider', 'procedure', 'amt']).apply(\n",
    "    lambda g: g['atheoretical'].corr(g['theoretical'])\n",
    ").unstack(level='amt')\n",
    "print(measure_convergence)\n",
    "\n",
    "\n",
    "# --- 2.2.3 Convergent Validity of Procedures (Between-Procedure) ---\n",
    "\n",
    "# This is the main hypothesis test: correlate measures between the two procedures.\n",
    "print(\"\\n\\n--- Between-Procedure Correlations (Convergent Validity) ---\")\n",
    "# Filter for the common reward amounts\n",
    "validity_df = individual_level_df[\n",
    "    ((individual_level_df['procedure'] == 'aa') & (individual_level_df['amt'] != 3)) |\n",
    "    ((individual_level_df['procedure'] == 'mcq') & (individual_level_df['amt'] != 2))\n",
    "].copy()\n",
    "validity_df['amount_label'] = np.where(validity_df['amt'] == 1, '$30', '$80')\n",
    "\n",
    "# Pivot to a wide format with one row per participant and columns for each measure/procedure\n",
    "validity_pivot = validity_df.pivot_table(\n",
    "    index=['id', 'provider', 'amount_label'],\n",
    "    columns='procedure',\n",
    "    values=['atheoretical', 'theoretical']\n",
    ")\n",
    "# Clean up the multi-level column names\n",
    "validity_pivot.columns = ['_'.join(col).strip() for col in validity_pivot.columns.values]\n",
    "\n",
    "# Calculate the between-procedure correlations\n",
    "validity_correlations = validity_pivot.groupby(['provider', 'amount_label']).apply(\n",
    "    lambda g: pd.Series({\n",
    "        'atheoretical_corr': g['atheoretical_aa'].corr(g['atheoretical_mcq']),\n",
    "        'theoretical_corr': g['theoretical_aa'].corr(g['theoretical_mcq'])\n",
    "    })\n",
    ")\n",
    "print(validity_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c653e",
   "metadata": {},
   "source": [
    "## 3. Comparing Systematic Differences in Discounting\n",
    "\n",
    "While the correlation analyses confirm that the two procedures measure the same construct, a crucial practical question remains: **are the measures interchangeable?** That is, do the two procedures and two online samples produce systematically different absolute values of discounting?\n",
    "\n",
    "To answer this, we replicate the final analysis from the paper. We fit a **linear mixed-effects model** to account for the repeated-measures design (i.e., multiple data points per participant). This model examines the main effects and interactions of **experimental procedure** (Adj-Amt vs. MCQ), **participant sample** (Prolific vs. MTurk), and **reward amount** on the estimated `log k` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "096b1532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Mixed-Effects Model: Comparing log k Values ---\n",
      "                                    Mixed Linear Model Regression Results\n",
      "=============================================================================================================\n",
      "Model:                              MixedLM                  Dependent Variable:                  theoretical\n",
      "No. Observations:                   1572                     Method:                              REML       \n",
      "No. Groups:                         393                      Scale:                               0.7341     \n",
      "Min. group size:                    4                        Log-Likelihood:                      -2549.8343 \n",
      "Max. group size:                    4                        Converged:                           Yes        \n",
      "Mean group size:                    4.0                                                                      \n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "                                                                  Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                                         -3.647    0.122 -29.843 0.000 -3.887 -3.408\n",
      "C(provider)[T.Prolific]                                           -1.161    0.198  -5.870 0.000 -1.549 -0.773\n",
      "C(amount_label)[T.80]                                             -0.337    0.078  -4.340 0.000 -0.490 -0.185\n",
      "C(procedure)[T.mcq]                                               -0.099    0.078  -1.276 0.202 -0.251  0.053\n",
      "C(provider)[T.Prolific]:C(amount_label)[T.80]                     -0.098    0.126  -0.780 0.435 -0.345  0.148\n",
      "C(provider)[T.Prolific]:C(procedure)[T.mcq]                        0.214    0.126   1.697 0.090 -0.033  0.460\n",
      "C(amount_label)[T.80]:C(procedure)[T.mcq]                         -0.356    0.110  -3.237 0.001 -0.571 -0.140\n",
      "C(provider)[T.Prolific]:C(amount_label)[T.80]:C(procedure)[T.mcq] -0.225    0.178  -1.262 0.207 -0.573  0.124\n",
      "Group Var                                                          2.896    0.297                            \n",
      "=============================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3.1 Mixed-Effects Model on Theoretical (log k) Measures ---\n",
    "\n",
    "# This analysis uses a linear mixed-effects model, the correct approach for a\n",
    "# repeated-measures design where each participant provides multiple data points.\n",
    "# This model accounts for the non-independence of observations by including a\n",
    "# random intercept for each participant.\n",
    "\n",
    "# Prepare the data for the model, ensuring categorical variables are set up\n",
    "anova_df = individual_level_df[\n",
    "    ((individual_level_df['procedure'] == 'aa') & (individual_level_df['amt'] != 3)) |\n",
    "    ((individual_level_df['procedure'] == 'mcq') & (individual_level_df['amt'] != 2))\n",
    "].copy()\n",
    "anova_df['amount_label'] = np.where(anova_df['amt'] == 1, '30', '80')\n",
    "\n",
    "# Define and fit the mixed-effects model\n",
    "# The `groups=anova_df['id']` term specifies the random intercepts for each participant.\n",
    "mixed_effects_model = smf.mixedlm(\n",
    "    \"theoretical ~ C(provider) * C(amount_label) * C(procedure)\",\n",
    "    data=anova_df,\n",
    "    groups=anova_df['id']\n",
    ").fit()\n",
    "\n",
    "# Print the model summary table\n",
    "print(\"--- Mixed-Effects Model: Comparing log k Values ---\")\n",
    "print(mixed_effects_model.summary())\n",
    "\n",
    "# --- Interpreting the Post-Hoc Test from the Summary Table ---\n",
    "# The original paper's post-hoc test examined the Procedure x Amount interaction.\n",
    "# In the summary table above, the interaction term:\n",
    "#   C(amount_label)[T.80]:C(procedure)[T.mcq]\n",
    "# directly tests if the difference between the MCQ and Adj-Amt procedures is\n",
    "# significantly different for the $80 amount compared to the $30 amount."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
